training:
  #batch size: [synth, weak, unlabel]
  batch_size: 128
  batch_size_val: 24
  n_epochs: 1 # max num epochs
  val_thresholds: [0.5] # thresholds used to compute f1 intersection in validation.
  enable_progress_bar: True
data: # change with your paths if different.
  # NOTE: if you have data in 44kHz only then synth_folder will be the path where
  # resampled data will be placed.
  metadata_file: "/nas/home/fronchini/EUSIPCO/urban-sound-class/UrbanSound8K/metadata/UrbanSound8K.csv"
  audio_dir: "/nas/home/fronchini/EUSIPCO/urban-sound-class/UrbanSound8K/audio"
  audio_max_len: 10
  fs: 22050
  net_subsample: 4
opt:
  lr: 0.001
feats:
  n_mels: 64
  n_filters: 2048
  hop_length: 512
  n_window: 1024
  sample_rate: 22050
  f_min: 0
  f_max: 8000
net:
  kernel_size: 3
  stride: 1
  padding: 2
  max_pooling_ks: 2
  nclass: 10

  in_channel: [1, 16, 32, 64]
  out_channel: [16, 32, 64, 128]
  
  # dropout: 0.5
  # rnn_layers: 2
  # attention: True
  # n_RNN_cell: 128
  # activation: glu
  # rnn_type: BGRU
  # kernel_size: [3, 3, 3, 3, 3, 3, 3]
  # padding: [1, 1, 1, 1, 1, 1, 1]
  # stride: [1, 1, 1, 1, 1, 1, 1]
  # nb_filters: [ 16, 32, 64, 128, 128, 128, 128 ]
  # pooling: [ [ 2, 2 ], [ 2, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ] ]
  # dropout_recurrent: 0
  # use_embeddings: False

        