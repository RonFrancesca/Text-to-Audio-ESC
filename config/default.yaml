fast_run: 1
processing: 'GPU'
#audio_used: 'both' # gen or original (according to if the audio used for training the network is original or generated)
gpu: "3"
allow_growth: "True"
base_dir: "/nas/home/fronchini/EUSIPCO/urban-sound-class"
log_dir: "/nas/home/fronchini/EUSIPCO/urban-sound-class/log_dir"
session_id: "test"
model: Cnn # Net or Cnn, according to the model that we want to initilize
concat_data: 0 # if contact data is flagged, then I am using both dataset, otherwise just the reale one
training:
  batch_size: 128
  batch_size_val: 64
  n_epochs: 100 # max num epochs
  val_thresholds: [0.5] # thresholds used to compute f1 intersection in validation.
testing:
  batch_size: 1
  mode: a # [a or f] for the entire dataset (all clip) and f is for frame by frame at validation and testing time
data: # change with your paths if different.
  # NOTE: if you have data in 44kHz only then synth_folder will be the path where
  # resampled data will be placed.
  metadata_file_real: "/nas/home/fronchini/EUSIPCO/urban-sound-class/UrbanSound8K/metadata/UrbanSound8K.csv"
  metadata_file_fake: "/nas/home/fronchini/EUSIPCO/urban-sound-class/audio_generation/AUDIOLDM2_dataset"
  audio_dir_real: "/nas/home/fronchini/EUSIPCO/urban-sound-class/UrbanSound8K/audio"
  audio_dir_fake: "/nas/home/fronchini/EUSIPCO/urban-sound-class/audio_generation/AUDIOLDM2_dataset"
  img: "/nas/home/fronchini/EUSIPCO/urban-sound-class/img"
  checkpoint_folder: "./checkpoint"
  audio_max_len: 4
  patch_lenght_s: 3
  normalization: "spec" # [spec or dataset], spec when normalization spec by spec, dataset when normalizing considering the whole dataset
opt:
  lr: 0.001
feats:
  n_mels: 64
  n_filters: 2048
  hop_length: 1024
  n_window: 1024
  sample_rate: 16000
  f_min: 0
  f_max: 8000
net:
  kernel_size: 5
  stride: 1
  padding: 2
  dense_drop: [0.5, 0.5]
  maxp_ks: [4, 2]
  maxp_stride: [4, 2]
  nclass: 10
  in_channel: [1, 24, 48]
  out_channel: [24, 48, 48]
  dense_in: [336, 64]
  dense_out: [64, 10]
  

        